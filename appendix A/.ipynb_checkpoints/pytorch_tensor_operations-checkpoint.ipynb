{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad6a1c5-f6a3-4a46-86ae-ceb234ffe372",
   "metadata": {},
   "source": [
    "# 附录1：pytorch的张量运算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74910eb-f5cd-4ee7-ade1-038667405218",
   "metadata": {},
   "source": [
    "pytorch是当前最为流行的深度学习框架。它包括4个对深度学习十分重要的功能：张量运算（torch)、自动梯度（torch.autograd）、神经网络（torch.nn）和优化方法（torch.optim）。张量运算是深度学习编程的基础。下面是基于我学习张量运算记录的笔记整理的一些有关pytorch张量运算的常用内容。\n",
    "\n",
    "## 1. 张量的基本概念\n",
    "张量是一个数学概念，在计算机编程中可以把张量简单理解成数组。\n",
    "1. 0维张量就是一个数，在数学上又称为“标量”；\n",
    "2. 1维张量就是1维数组，数学上又称为“向量”；\n",
    "3. 2维张量就是2维数组，在数学上又称为“矩阵”。矩阵的每一行称为行向量，每一列称为列向量。\n",
    "4. 3维及以上的张量就是多维数组，在数学直接称为张量。\n",
    "  \n",
    "神经网络模型处理的基本数据对象就是张量。\n",
    "1. 在大语言模型训练时的输入样本就是一个3维的张量。每个维度分别为：**批次的大小**，**文本序列长度**和**词向量的大小**。\n",
    "2. 在大语言模型多头注意机制中，值向量（values）就存储在一个4维张量中。每个维度分别为：**批次的大小**、**头的数量**、**文本序列长度**以及**每个值向量的大小**\n",
    "3. 图像卷积操作时被处理的图像一般也是一个三维张量。每个维度分别为：**图像的长**，**图像的宽**和**图像的通道数**。一般一个图像包括3个通道，分别对应R、G、B。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0a3a8f-5b25-4596-b1f0-bc5f48066810",
   "metadata": {},
   "source": [
    "## 2. 使用torch库的基本方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b232280c-8c20-4d27-82b0-778b94857566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用pytorch库的张量运算需要首先在程序中引入该库\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de13262-ad70-4549-ae5b-c17d68101266",
   "metadata": {},
   "source": [
    "调用torch库中张量函数的方法有两种：  \n",
    "1. 作为torch库的函数调用，\n",
    "2. 作为张量类的类函数调用。\n",
    "通常情况下这两种方法是等价。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b190551e-1610-4f8e-9fac-c8ad88197867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.]])\n",
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,3)\n",
    "y = torch.full((2,3), 2)\n",
    "#方法1：作为torch库的函数调用\n",
    "w = torch.add(x, y)\n",
    "#方法2：作为张量x类的类函数调用\n",
    "z = x.add(y)\n",
    "print(w)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05972ad-c7c0-419b-aa7f-475036318146",
   "metadata": {},
   "source": [
    "作为类函数有两种版本：  \n",
    "1. 结尾带下划线\n",
    "2. 结尾不带下划线\n",
    " \n",
    "不带下划线的版本将运算结果作为返回值，不修改对象本身。带下划线的版本直接修改对象本身。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe515c83-ab59-4c73-a92d-27c4eccb2064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "x=tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.]])\n",
      "z=tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,3)\n",
    "y = torch.full((2,3), 2)\n",
    "z = x.add(y)\n",
    "print(f\"x={x}\")\n",
    "z = x.add_(y)\n",
    "print(f\"x={x}\")\n",
    "print(f\"z={z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22555538-696b-4ea6-b292-8589ae94a5f5",
   "metadata": {},
   "source": [
    "## 3. torch库中常用的张量运算函数\n",
    "\n",
    "| 分类  | 函数 |  功能|\n",
    "| ---| --- | --- |\n",
    "|初始化张量 | torch.tensor | 出列表初始化 |\n",
    "|     |empty| 产生一个元素初值不确定的张量 |\n",
    "|     |zeros| 产生一个元素初值为0的张量 |\n",
    "|     |ones | 产生一个元素初值为1的张量 |\n",
    "|     |rand | 产生一个元素初值在（0, 1）之间均匀分布的张量|\n",
    "|     |randn | 产生一个元素初值按照均值为0，方差为1的标准正态分布的张量|\n",
    "|     |ones_like|产生一个形状和输入矩阵相同但元素初值为1的张量|\n",
    "|     |randn_like|产生一个形状和输入矩阵相同但元素初值按照均值为0，方差为1的标准正态分布的张量|\n",
    "|     |linespace|产生在指定区间（含端点）内等距取值的一维向量|\n",
    "|改变张量形状|unsqueeze|在指定位置增加1个大小为1的维度|\n",
    "|          |squeeze|在指定为减少1个维度，只能减少大小为1的维度|\n",
    "|          |transpose|交换张量的两个维度|\n",
    "|          |view|将张量改为指定形状，不改变内存中数据的位置|\n",
    "|          |reshape|将张量改为指定形状，必要时改变内存中数据的位置|\n",
    "|张量的合并与拆分|cat|将多个张量沿着某个维度合并|\n",
    "|    |stack|沿着一个新的维度将多个张量合并|\n",
    "|    |split|将张量沿某个维度，拆分成多个张量|\n",
    "|    |chunk|将张量沿某个指定的维度切成若干块|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd39b6-6670-4020-980a-1c0cf77a62c7",
   "metadata": {},
   "source": [
    "## 4. 初始化张量  \n",
    "### 4.1 将python list转化为张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a99d547d-3d26-4264-8973-d9cf7e83a12d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4])\n",
      "torch.float64\n",
      "tensor([[1., 2., 3., 4.],\n",
      "        [5., 6., 7., 8.],\n",
      "        [2., 4., 6., 8.]], dtype=torch.float64)\n",
      "[[1.0, 2.0, 3.0, 4.0], [5.0, 6.0, 7.0, 8.0], [2.0, 4.0, 6.0, 8.0]]\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3,4],[5,6,7,8],[2,4,6,8]], dtype=torch.float64)\n",
    "print(x.shape)\n",
    "print(x.dtype)\n",
    "print(x)\n",
    "y = x.tolist() #把张量转化为list\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152d93e0-de79-4244-8ce2-27de8a915a6a",
   "metadata": {},
   "source": [
    "### 4.2 利用函数生成张量\n",
    "### 4.2.1 指定张量元素的初值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e6b3acb2-52cb-470e-8600-ada1fdba9b46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([[[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]],\n",
      "\n",
      "        [[0., 0.],\n",
      "         [0., 0.],\n",
      "         [0., 0.]]])\n",
      "b=tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "c=tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 1],\n",
      "        [1, 1, 1, 1]])\n",
      "d=tensor([[3.1400, 3.1400, 3.1400],\n",
      "        [3.1400, 3.1400, 3.1400]])\n",
      "e=tensor([[3.1400, 3.1400, 3.1400],\n",
      "        [3.1400, 3.1400, 3.1400]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.empty(2, 3, 2) #用empty函数生成张量。注意元素值取值是不确定的\n",
    "b = torch.zeros(4,2) #创建各元素值都是0的张量\n",
    "c = torch.ones(3, 4, dtype=torch.int64) #创建各元素都是1的张量\n",
    "d = torch.full((2, 3), 3.14) #给张量元素指定任意初值\n",
    "e = torch.empty((2, 3)).fill_(3.14) #给张量元素指定任意初值的另一种方法\n",
    "print(f\"a={a}\")\n",
    "print(f\"b={b}\")\n",
    "print(f\"c={c}\")\n",
    "print(f\"d={d}\")\n",
    "print(f\"e={e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356471c5-e973-46b1-9fbc-ed4bc0198b18",
   "metadata": {},
   "source": [
    "#### 4.2.2 生成初值符合某种条件的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9da1c7c5-21d4-4af3-97dd-333d303336d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f=tensor([[[-0.5926, -1.8964],\n",
      "         [ 1.0145, -0.3874],\n",
      "         [-0.6223, -0.0522],\n",
      "         [ 0.3126,  0.4287]],\n",
      "\n",
      "        [[-0.5801,  0.6547],\n",
      "         [ 0.3417, -0.7546],\n",
      "         [-0.1749, -1.5067],\n",
      "         [ 0.8497, -0.9615]]])\n",
      "g=tensor([[0.2595, 0.2191, 0.5739],\n",
      "        [0.5101, 0.4375, 0.7918]])\n",
      "h=tensor([ 1.0000,  3.2500,  5.5000,  7.7500, 10.0000])\n"
     ]
    }
   ],
   "source": [
    "f = torch.randn(2,4,2) #创建元素取值符合均值为0，方差为1的标准高斯分布的张量\n",
    "g = torch.rand(2,3) #创建元素值在（0,1）之间且满足均匀分布的张量\n",
    "h = torch.linspace(1, 10, 5) #生成1维张量，元素值从左到右依次从1到10的区间里等距离采样\n",
    "print(f\"f={f}\")\n",
    "print(f\"g={g}\")\n",
    "print(f\"h={h}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c19ea22e-9eb2-4524-a592-77f47d219ce4",
   "metadata": {},
   "source": [
    "#### 4.2.3 生成形状和另一个张量相同的张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2672c9d4-0742-408e-a5e8-b6e9b5a46119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i=tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "j=tensor([[ 0.8391, -0.1910, -2.4852],\n",
      "        [-0.7231, -0.5464, -0.6859]])\n"
     ]
    }
   ],
   "source": [
    "i = torch.ones_like(d) #创建一个形状和d相同，元素值全为1的张量\n",
    "j = torch.randn_like(d) #创建一个形状和d相同,元素值符合均值为0，方差为1的标准高斯分布\n",
    "print(f\"i={i}\")\n",
    "print(f\"j={j}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5771cb4a-cb87-4bb7-bb71-fcc1e5ee834a",
   "metadata": {},
   "source": [
    "## 5. 访问张量  \n",
    "### 5.1 通过索引访问张量中的某个元素。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "74c6cc8f-eed2-4e13-8544-d1850170672f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a=tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n",
      "tensor(7)\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(0,10)\n",
    "print(f\"a={a}\")\n",
    "x = a[7]\n",
    "print(x)\n",
    "print(x.item())#当张量只包含一个元素时，用item函数返回这个值。注意比较两个输出在类型上的区别。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d6b4375-662b-45e9-b108-e15b78702957",
   "metadata": {},
   "source": [
    "### 5.2 通过索引获得张量的某个切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "412e4d71-6865-4ad5-96a2-8583cc918fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 5, 6, 7])\n",
      "tensor([7, 8])\n",
      "tensor([6, 7, 8, 9])\n",
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n",
      "torch.Size([9, 10])\n",
      "torch.Size([9, 2, 9])\n"
     ]
    }
   ],
   "source": [
    "b = a[4:8]\n",
    "c = a[7:-1]\n",
    "d = a[6:]\n",
    "e = a[:-1]\n",
    "f = torch.randn(9,8,10)\n",
    "g = f[:,2,:]\n",
    "h = f[:,3:5,:-1]\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(g.size())\n",
    "print(h.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f453b5-a940-4bae-9896-1eda6b033b51",
   "metadata": {},
   "source": [
    "## 6. 改变张量的形状 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6f7324-999d-4d40-8c25-4b27c3ebe093",
   "metadata": {},
   "source": [
    "### 6.1 增加张量的维度（torch.unsqueeze） "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "70bd674d-f7d4-403f-9b3e-4f1068770d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1])\n",
      "tensor([[[-0.8945],\n",
      "         [-1.3630],\n",
      "         [-0.0504]],\n",
      "\n",
      "        [[-0.7699],\n",
      "         [ 0.9950],\n",
      "         [ 0.8197]]])\n",
      "torch.Size([2, 1, 3])\n",
      "tensor([[[-0.8945, -1.3630, -0.0504]],\n",
      "\n",
      "        [[-0.7699,  0.9950,  0.8197]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "b = a.unsqueeze(-1) #在张量a的最后增加一个维度，大小为1\n",
    "c = a.unsqueeze(1) #在张量a的第1个维度之后增加一个维度，大小为1\n",
    "print(b.size())\n",
    "print(b)\n",
    "print(c.size())\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02dcef9-70d3-4b6d-bc3f-d0d9f05f9ca2",
   "metadata": {},
   "source": [
    "### 6.2 减少张量的维度（torch.squeeze)\n",
    "在张量的指定位置减少一个维度，要求被减少的维度大小为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "edde3b65-dc1c-4eb5-9137-0001c2764c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "tensor([[-0.8945, -1.3630, -0.0504],\n",
      "        [-0.7699,  0.9950,  0.8197]])\n"
     ]
    }
   ],
   "source": [
    "d = b.squeeze(-1) #压缩张量b的最后1维。被压缩的维度对应的大小只能是1，否则不压缩\n",
    "print(d.size())\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c81ae-ccf1-4fb7-9dcd-879daa059fce",
   "metadata": {},
   "source": [
    "### 6.3 交换张量的维度（torch.tranpose）  \n",
    "交换张量的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca3c65c1-fdcd-42b3-993b-c6cd214edc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 6, 9, 1, 3, 7])\n"
     ]
    }
   ],
   "source": [
    "p = torch.rand(5,6,3,1,9,7)\n",
    "p = p.transpose(2,4)\n",
    "print(p.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8091cdb1-15ab-4a54-a899-3e616afdb3ce",
   "metadata": {},
   "source": [
    "### 6.4 任意改变张量的形状（torch.view和torch.reshape）\n",
    "view和reshape都可以灵活的改变张量的形状，只要改变后的各维度大小相乘和改变前各维度大小相乘相等即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7a3baaf-26c2-4c2a-9cb4-6458725b684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 81, 2, 4])\n",
      "torch.Size([90, 9, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "p = torch.rand(5,162,8)\n",
    "q = p.view((10, 81, 2, 4))\n",
    "r = p.reshape((90, 9, 4, 2))\n",
    "print(q.size())\n",
    "print(r.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ad6e08-2580-4fcb-a9e3-c437f9d61e58",
   "metadata": {},
   "source": [
    "view和reshape改变张量形状时，并非真正改变了数据在内存中的位置，而是改变张量的索引。这样做减少了数据在内存的复制，因此比较高效，需要张量在变形之前索引是连续的。如果索引不是连续的，则只能使用reshape函数来改变张量的形状。关于这一点的具体解释可参见下面第8小节"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a18de21-cf53-4959-a196-2344fddefd66",
   "metadata": {},
   "source": [
    "## 7. 张量的合并和拆分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b4275-d53f-4174-bf0c-d2c0c3904a50",
   "metadata": {},
   "source": [
    "### 7.1 张量的合并（torch.cat和torch.stack）  \n",
    "torch.cat，将多个张量沿着某个维度合并。要求合并的张量除了这个维度之外，其他维度的大小必须一样。\n",
    "torch.stack，沿着一个新的维度将多个张量合并。要求这些张量的形状必须完全一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "243ce915-db26-4d31-8d92-93e38ff7c35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7, 5, 6])\n",
      "torch.Size([2, 3, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3,5,6)\n",
    "b = torch.randn(2,4,5,6)\n",
    "#使用cat函数，沿指定维度合并张量。把需要合并在张量放在一个tuple里。\n",
    "c = torch.cat((a, b), dim=1)\n",
    "print(c.size())\n",
    "d = torch.randn(2,3,5,6)\n",
    "e = torch.randn(2,3,5,6)\n",
    "f = torch.randn(2,3,5,6)\n",
    "g = torch.randn(2,3,5,6)\n",
    "#使用stack函数，将a, b, c, d 4个张量合并成新张量的第3个维度\n",
    "h = torch.stack((d, e, f, g), dim=2)\n",
    "print(f.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e39ecf-a523-45b6-ab21-4ee39312d113",
   "metadata": {},
   "source": [
    "### 7.2 张量的拆分 \n",
    "split函数将张量沿某个维度，拆分成多个张量。\n",
    "chunk函数将张量沿某个指定的维度切成若干块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "22938edd-4a5e-403c-8797-837f121b36d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 7])\n",
      "torch.Size([5, 50, 7])\n",
      "torch.Size([5, 40, 7])\n",
      "torch.Size([5, 62, 7])\n",
      "torch.Size([5, 41, 7])\n",
      "torch.Size([5, 41, 7])\n",
      "torch.Size([5, 41, 7])\n",
      "torch.Size([5, 39, 7])\n"
     ]
    }
   ],
   "source": [
    "p =torch.randn(5,162,7)\n",
    "a, b, c, d = torch.split(p, [10, 50, 40, 62], dim=1)\n",
    "print(a.size())\n",
    "print(b.size())\n",
    "print(c.size())\n",
    "print(d.size())\n",
    "e, f, g, h = torch.chunk(p, 4, dim=1)#沿第2维将张量分成4块，4块大小不一定一样。\n",
    "print(e.size())\n",
    "print(f.size())\n",
    "print(g.size())\n",
    "print(h.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf14c9d-63af-417d-8b04-d441f4483d29",
   "metadata": {},
   "source": [
    "## 8. 广播机制\n",
    "广播机制用于解决形状不匹配的张量间进行运算的问题。概括的的说，广播机制会按照规则，逐一比较两个向量的各个维度，并通过复制的方式将较小的维度扩大到和较大维度相等，从而最终使两者形状一致。需要注意的是并非任意两个张量都可以通过广播变成形状一致。可以通过广播机制变为形状一致的张量需满足以下条件：  \n",
    "1. 两个张量都不能是0维的。\n",
    "2. 从右向左逐个维度比较两个张量。两个张量各对应维度需要满足以下条件之一  \n",
    "    . 相等  \n",
    "    . 其中一个张量的大小为1，可通过复制变为一致   \n",
    "    . 其中一个张量不存在这个维度，可通过插入维度在复制变为一致"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77215b80-028f-466e-9fb7-b07c2343a16b",
   "metadata": {},
   "source": [
    "a和b可通过广播变为形状一致：\n",
    "从右向左依次\n",
    "1. 第1个维度，b维度大小是1\n",
    "2. 第2个维度，a和b的维度大小相同\n",
    "3. 第3个维度，a的维度大小是1\n",
    "4. 第4个维度：b该维度不存在"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b9a4d8-7a1d-42fb-9bad-3149ccfcb125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2, 1, 4, 3)\n",
    "b = torch.randn(   5, 4, 1)\n",
    "c = a + b\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3846bc42-232a-4820-bbc7-78699f7100f2",
   "metadata": {},
   "source": [
    "下面代码中的x和y不可通过广播变为形状一致，因为从右边数第3个维度，两个张量该维度的大小不同且没有一个张量该维度的大小为1，无法通过复制的方式让它们的维度一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2477898-9d54-443c-b7d1-50c0d3876e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.empty(2, 2, 4, 3)\n",
    "y = torch.empty(   5, 4, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69be1e91-3609-4ce1-9911-b9709de36216",
   "metadata": {},
   "source": [
    "利用广播机制完成张量加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74356aae-2d8c-4ccf-92ae-34d8906d6d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 5])\n",
      "tensor([[[3.1000, 3.1000, 3.1000, 3.1000, 3.1000],\n",
      "         [3.1000, 3.1000, 3.1000, 3.1000, 3.1000]],\n",
      "\n",
      "        [[3.1000, 3.1000, 3.1000, 3.1000, 3.1000],\n",
      "         [3.1000, 3.1000, 3.1000, 3.1000, 3.1000]],\n",
      "\n",
      "        [[3.1000, 3.1000, 3.1000, 3.1000, 3.1000],\n",
      "         [3.1000, 3.1000, 3.1000, 3.1000, 3.1000]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(3, 2, 1)\n",
    "b = torch.empty(5).fill_(2.1)\n",
    "c = a + b\n",
    "print(c.size())\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f853c382-5db1-4fb0-b553-3ede4ace72c7",
   "metadata": {},
   "source": [
    "## 9. 张量的乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d537f66f-1364-4baf-b820-18ac5afed537",
   "metadata": {},
   "source": [
    "### 9.1 逐元素相乘（*）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "85e9ba6d-6640-4797-ae3d-f442fc477f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6., 6., 6.],\n",
      "        [6., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.empty((2,3)).fill_(2.0)\n",
    "b = torch.empty((1,3)).fill_(3.0)\n",
    "#不要求输入两个张量形状一致，但两个张量需要能够通过广播机制变为形状一致。\n",
    "c = a * b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf69853-9069-40de-bd1b-addd70039110",
   "metadata": {},
   "source": [
    "### 9.2 向量点积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "218ab523-2490-4866-8e26-2d076c1d20a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30)\n"
     ]
    }
   ],
   "source": [
    "a = torch.full((5,), 2)\n",
    "b = torch.full((5,), 3)\n",
    "#输入必须都是1维向量，且维度大小一致。\n",
    "c = torch.dot(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb796db-d216-477b-ac5b-787fa338c257",
   "metadata": {},
   "source": [
    "### 9.3 向量外积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce8a7b85-ec8f-46fd-9521-223c2450d12b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6, 6, 6],\n",
      "        [6, 6, 6],\n",
      "        [6, 6, 6]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.full((3,), 2)\n",
    "b = torch.full((3,), 3)\n",
    "#输入必须都是1维向量，且维度大小一致。\n",
    "c = torch.outer(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936817eb-862b-4fca-b9f8-180afcc863d1",
   "metadata": {},
   "source": [
    "### 9.4 矩阵乘法（@）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a3bf097f-4e32-4e72-ad5c-2e0b19e8ce91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18., 18.],\n",
      "        [18., 18.]])\n"
     ]
    }
   ],
   "source": [
    "#两个2维矩阵相乘，维度必须匹配，即前一个矩阵的列数必须和后一个矩阵的行数相等\n",
    "a = torch.empty((2,3)).fill_(2.0)\n",
    "b = torch.empty((2,3)).fill_(3.0)\n",
    "c = a @ b.transpose(0,1)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cc01421a-1842-4466-ac43-38ea76b1f258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 6.])\n"
     ]
    }
   ],
   "source": [
    "#向量和矩阵相乘，相当于行向量和矩阵相乘。\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.ones(3,2)\n",
    "c = a @ b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "675e4658-5a01-47f4-ad3f-523c097e7a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 5., 5.])\n"
     ]
    }
   ],
   "source": [
    "#矩阵和向量相乘，相当于矩阵和列向量相乘\n",
    "a = torch.ones(3,2)\n",
    "b = torch.tensor([1.0,4.0])\n",
    "c = a @ b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a2399e-4c90-44d3-b82e-9e75a53b9f81",
   "metadata": {},
   "source": [
    "### 9.5 批量矩阵乘法（@）\n",
    "当相乘的张量维度大于2时，执行批量矩阵乘法。对于形状为(..., m, n)的张量A和形状为(..., n, p)的张量B，批量矩阵乘法的结果是一个形状为(..., m, p)的张量。每个张量的\"...\"部分代表任意数量的批量维度。当进行批量乘法时，pytorch会利用广播机制自动让各向量批量维度一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d4ccc329-108f-4109-be6b-2866f339d9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 3])\n",
      "tensor([[[ 0.1684,  0.9406, -1.7558],\n",
      "         [-2.6504,  1.8649,  2.7411],\n",
      "         [ 5.1198, -4.5485, -3.3203],\n",
      "         [ 3.4810, -6.1789, -4.1025]],\n",
      "\n",
      "        [[ 2.4855, -6.3756, -2.7549],\n",
      "         [-5.1606,  3.1573,  1.1740],\n",
      "         [-0.2549,  3.1623,  3.0671],\n",
      "         [ 2.1162, -5.3233, -1.2320]],\n",
      "\n",
      "        [[-0.1310,  3.3769, -0.3214],\n",
      "         [-3.0650,  1.6578, -0.8541],\n",
      "         [-0.9523,  0.0756, -0.4481],\n",
      "         [-1.1875,  2.6943,  2.6880]]])\n"
     ]
    }
   ],
   "source": [
    "d = torch.randn(3,4,5)\n",
    "e = torch.randn(1,5,3)\n",
    "f = d @ e\n",
    "print(f.size())\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64127c9e-3495-4134-9e82-b94d7d57f01e",
   "metadata": {},
   "source": [
    "### 9.6 利用矩阵乘法实现批量向量内积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c2087d81-a8dc-49cd-9430-6966dcce49a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "tensor([[-2.8519,  1.6670],\n",
      "        [ 3.2973,  4.3137],\n",
      "        [-0.4093,  0.5068]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3,2,5)\n",
    "b = torch.randn(3,2,5)\n",
    "#对a和b的最后一维做批量内积\n",
    "a = a.unsqueeze(-2) #a形状为(3, 2, 1, 5) \n",
    "b = b.unsqueeze(-1) #b形状为(3, 2, 5, 1)\n",
    "c = a @ b #c形状为(3, 2, 1, 1)\n",
    "c.squeeze_(-1).squeeze_(-1)\n",
    "print(c.size())\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c64779-b518-40e7-a3a7-33517f5bd1e9",
   "metadata": {},
   "source": [
    "### 9.7 利用矩阵乘法实现批量向量外积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5f96fc76-a449-4e72-84be-660e4dc26427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 2, 2])\n",
      "tensor([[[[-0.5924, -1.8389],\n",
      "          [-0.3113, -0.9664]],\n",
      "\n",
      "         [[ 0.7701, -0.0239],\n",
      "          [-0.1870,  0.0058]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4913, -0.2648],\n",
      "          [-1.2844,  0.6923]],\n",
      "\n",
      "         [[-0.1414,  0.0411],\n",
      "          [-1.3858,  0.4028]]],\n",
      "\n",
      "\n",
      "        [[[-0.9607,  1.4274],\n",
      "          [ 1.1836, -1.7586]],\n",
      "\n",
      "         [[ 0.0319, -0.0793],\n",
      "          [ 0.5575, -1.3866]]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3,2,2)\n",
    "b = torch.randn(3,2,2)\n",
    "#对a和b的最后一维做批量外积\n",
    "a = a.unsqueeze(-1) #a形状为(3, 2, 2, 1) \n",
    "b = b.unsqueeze(-2) #b形状为(3, 2, 1, 2)\n",
    "c = a * b #这里利用了广播机制，c的形状为(3, 2, 2, 2)\n",
    "print(c.size())\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927fc73-42a1-47e9-9741-01a060f716f9",
   "metadata": {},
   "source": [
    "## 10. 爱因斯坦求和约定（torch.einsum)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8d56b9-c60e-4e2e-84c8-edfa13d6431f",
   "metadata": {},
   "source": [
    "einsum是一个十分强大的函数，可以简明的完成复杂的张量运算。\n",
    "### 10.1 张量扩维\n",
    "'i, j -> ij' 相当于 $a^Tb$，也就是向量a和向量b的元素两两相乘得到一个矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "32de5c07-9154-48eb-874c-a86d7e8e9825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6, 6],\n",
      "        [6, 6],\n",
      "        [6, 6]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.full((3,),2)\n",
    "b = torch.full((2,),3)\n",
    "c = torch.einsum('i, j -> ij', a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6725d1fd-6ae3-430c-bd05-ed11422f01d6",
   "metadata": {},
   "source": [
    "### 10.2 张量缩维\n",
    "'i->'相当于对该维度求和$\\sum_{i}a_{i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a1b69df3-024d-4a39-9569-4acc6fa0f12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n"
     ]
    }
   ],
   "source": [
    "d = torch.full((3,),2)\n",
    "e = torch.einsum('i->', d)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd3d598-e373-4355-bd4e-23246def3e50",
   "metadata": {},
   "source": [
    "### 10.3 张量的交换不变性\n",
    "交换张量任意两个维度，不改变张量本身。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6fdb59cd-a8fd-481c-8bed-d1548b2349e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8026,  1.5833,  0.8638],\n",
      "        [-0.7444,  0.3484,  0.2103]])\n",
      "tensor([[ 0.8026, -0.7444],\n",
      "        [ 1.5833,  0.3484],\n",
      "        [ 0.8638,  0.2103]])\n"
     ]
    }
   ],
   "source": [
    "f = torch.randn(2,3)\n",
    "g = torch.einsum('ij->ji', f)\n",
    "print(f)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68dac8d6-eb6d-459c-86c3-b9f6a259e7ec",
   "metadata": {},
   "source": [
    "### 10.4 逐元素相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "66dc3b54-b28d-4eca-8310-c738c25765a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.5000, 1.5000, 1.5000],\n",
      "         [1.5000, 1.5000, 1.5000],\n",
      "         [1.5000, 1.5000, 1.5000]],\n",
      "\n",
      "        [[1.5000, 1.5000, 1.5000],\n",
      "         [1.5000, 1.5000, 1.5000],\n",
      "         [1.5000, 1.5000, 1.5000]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,3,3)\n",
    "b = torch.empty(2,3,3).fill_(1.5)\n",
    "c = torch.einsum('ijk, ijk -> ijk', a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9dc2c4-b971-4396-bcea-0c39f6a3a013",
   "metadata": {},
   "source": [
    "### 10.5 矩阵乘法 \n",
    "'ij, jk -> ik' 相当于 \n",
    "1. 交换不变性'ij -> ji'；\n",
    "2. 张量扩维：'ji, jk -> jik'\n",
    "3. 张量缩维：'jik->ik'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "777bf88e-ba9d-4f8b-9fe7-a5a8a0521199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18, 18],\n",
      "        [18, 18]])\n",
      "tensor([[18, 18],\n",
      "        [18, 18]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.full((2,3),2)\n",
    "b = torch.full((3,2),3)\n",
    "c = a @ b\n",
    "d = torch.einsum('ij, jk -> ik', a, b)\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246991b-3816-4b8d-a8fa-e5b948dc4982",
   "metadata": {},
   "source": [
    "### 10.6 向量点积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e6376b68-ddf3-4467-9262-cef5cc09353f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18)\n",
      "tensor(18)\n"
     ]
    }
   ],
   "source": [
    "a = torch.full((3,),2)\n",
    "b = torch.full((3,),3)\n",
    "c = torch.dot(a,b)\n",
    "d = torch.einsum('i,i ->', a, b)\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c27e79-51b5-415e-b0c8-3e49b9c58a7c",
   "metadata": {},
   "source": [
    "### 10.7 向量外积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8c74e7fc-ff5c-47be-9f4c-e6f7ebc1fffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6, 6, 6],\n",
      "        [6, 6, 6],\n",
      "        [6, 6, 6]])\n"
     ]
    }
   ],
   "source": [
    "d = torch.einsum('i,j -> ij', a, b)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2daf3266-57bb-488d-a15b-2bee8b209549",
   "metadata": {},
   "source": [
    "### 10.7 高维张量运算\n",
    "einsum函数的最强大之处在于可以简明的完成高维张量的运算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7cb22d2a-639d-42f1-b271-d881d4cdbb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3, 4])\n",
      "tensor([[[[0.7740, 0.5066, 0.5911, 0.5199],\n",
      "          [1.4847, 0.8234, 0.7827, 1.0130],\n",
      "          [1.6655, 1.1336, 1.1268, 1.0377]],\n",
      "\n",
      "         [[1.6491, 1.6983, 1.1284, 1.3408],\n",
      "          [1.3859, 1.9514, 1.1355, 1.4963],\n",
      "          [1.9382, 1.9166, 1.4310, 1.5330]],\n",
      "\n",
      "         [[0.8259, 0.5468, 0.2716, 0.8515],\n",
      "          [0.9532, 0.7827, 0.1580, 0.7060],\n",
      "          [2.0262, 1.9009, 0.8077, 2.0046]]],\n",
      "\n",
      "\n",
      "        [[[1.3597, 1.1800, 1.1179, 1.1697],\n",
      "          [1.0490, 1.4462, 0.8551, 1.4018],\n",
      "          [1.3576, 1.3353, 1.2436, 1.5512]],\n",
      "\n",
      "         [[1.1342, 0.6835, 0.7239, 0.6689],\n",
      "          [1.0150, 0.4593, 0.5466, 0.4720],\n",
      "          [1.0565, 0.6824, 0.7125, 0.6248]],\n",
      "\n",
      "         [[0.9407, 1.2459, 2.2897, 1.4569],\n",
      "          [0.9277, 0.8407, 1.7704, 1.4559],\n",
      "          [0.7586, 0.9235, 1.4937, 1.0372]]]])\n"
     ]
    }
   ],
   "source": [
    "#两个张量最后一维做外积，再沿着l维求和。\n",
    "a = torch.rand(2, 3, 5, 3)\n",
    "b = torch.rand(2, 3, 5, 4)\n",
    "c = torch.einsum('bhlk, bhlv->bhkv', a, b)\n",
    "print(c.shape)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5b517-d2bd-4f6e-a6ab-078f09cc2dfe",
   "metadata": {},
   "source": [
    "下面这段代码通过转置和矩阵乘法运算达到和上面爱因斯坦求和约定相同的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2af8da59-6c4e-4d88-9459-df5acf36f00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3, 4])\n",
      "tensor([[[[0.7740, 0.5066, 0.5911, 0.5199],\n",
      "          [1.4847, 0.8234, 0.7827, 1.0130],\n",
      "          [1.6655, 1.1336, 1.1268, 1.0377]],\n",
      "\n",
      "         [[1.6491, 1.6983, 1.1284, 1.3408],\n",
      "          [1.3859, 1.9514, 1.1355, 1.4963],\n",
      "          [1.9382, 1.9166, 1.4310, 1.5330]],\n",
      "\n",
      "         [[0.8259, 0.5468, 0.2716, 0.8515],\n",
      "          [0.9532, 0.7827, 0.1580, 0.7060],\n",
      "          [2.0262, 1.9009, 0.8077, 2.0046]]],\n",
      "\n",
      "\n",
      "        [[[1.3597, 1.1800, 1.1179, 1.1697],\n",
      "          [1.0490, 1.4462, 0.8551, 1.4018],\n",
      "          [1.3576, 1.3353, 1.2436, 1.5512]],\n",
      "\n",
      "         [[1.1342, 0.6835, 0.7239, 0.6689],\n",
      "          [1.0150, 0.4593, 0.5466, 0.4720],\n",
      "          [1.0565, 0.6824, 0.7125, 0.6248]],\n",
      "\n",
      "         [[0.9407, 1.2459, 2.2897, 1.4569],\n",
      "          [0.9277, 0.8407, 1.7704, 1.4559],\n",
      "          [0.7586, 0.9235, 1.4937, 1.0372]]]])\n"
     ]
    }
   ],
   "source": [
    "x= a.transpose(-2,-1)\n",
    "y = (x @ b)\n",
    "print(y.shape)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f71101d5-1593-4a96-a2e9-9691b823c080",
   "metadata": {},
   "source": [
    "下面这段代码通过逐元素相乘再对张量按照某个维度求和，同样达到和einsum函数相同的效果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "8fa10f82-83d8-4928-87ba-2d761a108ffd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3, 4])\n",
      "tensor([[[[0.7740, 0.5066, 0.5911, 0.5199],\n",
      "          [1.4847, 0.8234, 0.7827, 1.0130],\n",
      "          [1.6655, 1.1336, 1.1268, 1.0377]],\n",
      "\n",
      "         [[1.6491, 1.6983, 1.1284, 1.3408],\n",
      "          [1.3859, 1.9514, 1.1355, 1.4963],\n",
      "          [1.9382, 1.9166, 1.4310, 1.5330]],\n",
      "\n",
      "         [[0.8259, 0.5468, 0.2716, 0.8515],\n",
      "          [0.9532, 0.7827, 0.1580, 0.7060],\n",
      "          [2.0262, 1.9009, 0.8077, 2.0046]]],\n",
      "\n",
      "\n",
      "        [[[1.3597, 1.1800, 1.1179, 1.1697],\n",
      "          [1.0490, 1.4462, 0.8551, 1.4018],\n",
      "          [1.3576, 1.3353, 1.2436, 1.5512]],\n",
      "\n",
      "         [[1.1342, 0.6835, 0.7239, 0.6689],\n",
      "          [1.0150, 0.4593, 0.5466, 0.4720],\n",
      "          [1.0565, 0.6824, 0.7125, 0.6248]],\n",
      "\n",
      "         [[0.9407, 1.2459, 2.2897, 1.4569],\n",
      "          [0.9277, 0.8407, 1.7704, 1.4559],\n",
      "          [0.7586, 0.9235, 1.4937, 1.0372]]]])\n"
     ]
    }
   ],
   "source": [
    "a.unsqueeze_(-1)\n",
    "b.unsqueeze_(-2)\n",
    "c = (a * b).sum(-3)\n",
    "print(c.size())\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1953e00-67ff-45f8-85c6-bc386b099bbf",
   "metadata": {},
   "source": [
    "### 10.9 einsum实战例子\n",
    "用einsum函数重写Qwen3-next-80B源码文件module_qwen3_next.py中函数torch_recurrent_gated_delta_rule中的代码。下面是源代码"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8f18ee8-1773-412f-9fca-335b01c212dd",
   "metadata": {},
   "source": [
    "last_recurrent_state = last_recurrent_state + k_t.unsqueeze(-1) * delta.unsqueeze(-2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157512e9-0176-4e28-b36a-c3a829c0c50e",
   "metadata": {},
   "source": [
    "下面是改写之后的代码。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7ae134d-9d41-4011-a394-593fb0ae6b9e",
   "metadata": {},
   "source": [
    "last_recurrent_state +=  torch.einsum('bhlk, bhlv -> bhkv', k_t, delta）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ba35b6-7d00-4bed-9006-0a9b6dc34cac",
   "metadata": {},
   "source": [
    "## 11. 张量在内存中的存储\n",
    "张量的数据在内存中是连续存储的。张量的形状只是不同索引方式而已。不同张量可以指向同一个存储区。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "50d0dbc0-7325-451f-9efa-6265fc8d5a7c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.3235, -1.2144,  1.6674],\n",
      "        [-0.5466,  0.5778, -0.0047]])\n",
      "4567721824\n",
      "4567721824\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " 0.3235391080379486\n",
       " -1.2144300937652588\n",
       " 1.6673688888549805\n",
       " -0.5466006398200989\n",
       " 0.5777862668037415\n",
       " -0.004692655988037586\n",
       "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn((2,3))\n",
    "print(a)\n",
    "b = a\n",
    "print(id(a)) #id函数返回张量指向的地址空间\n",
    "print(id(b))\n",
    "storage = a.storage()#storage函数返回张量存储区是实际内容\n",
    "storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58c87053-5391-487d-80f8-a42252e2a987",
   "metadata": {},
   "source": [
    "view函数只是改变了张量的索引方式，并未改变张量在内存中存储的方式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "81b3002a-4fc2-4d9f-9faf-db2a6a6dba51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      " 0.3235391080379486\n",
      " -1.2144300937652588\n",
      " 1.6673688888549805\n",
      " -0.5466006398200989\n",
      " 0.5777862668037415\n",
      " -0.004692655988037586\n",
      "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]\n"
     ]
    }
   ],
   "source": [
    "c = a.view(3, 2)\n",
    "print(c.size())\n",
    "print(c.storage())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45975ab-50fb-402f-b643-62587f7db7b6",
   "metadata": {},
   "source": [
    "如果我们把张量中所有元素的索引按照下面规则排成一行。**按照维度从左向右，比较索引对应的下标值，标值小的排在前面。**  这时，如果该索引序列和每个索引对饮元素在内存中实际排列一致，我们就说索引是连续的。view函数和reshape函数虽然改变张量的形状，其实只是改变了索引结构，不会破坏索引的连续性，也不需要改变元素在内存中的位置。有些张量变换，比如交换张量维度，会破坏张索引的连续性。一旦索引的连续性被破坏就不能再使用view函数来改变张量的形状，必须使用reshape函数。这时候reshape函数会通过改变内存中元素的位置来回复索引的连续性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5b2889ac-31ed-4fab-abf0-2f831e8d2d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "torch.Size([3, 10, 4])\n",
      "True\n",
      "torch.Size([3, 4, 2, 5])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#函数is_contiguous用于判断当前张量的索引是否连续。\n",
    "a = torch.randn(3,5,2,4)\n",
    "print(a.is_contiguous())\n",
    "b = a.view(3,10,4)\n",
    "print(b.size())\n",
    "print(b.is_contiguous())\n",
    "c = a.transpose(1,3) #破坏了数据的连续性\n",
    "print(c.size())\n",
    "print(c.is_contiguous())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
