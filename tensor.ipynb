{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ad6a1c5-f6a3-4a46-86ae-ceb234ffe372",
   "metadata": {},
   "source": [
    "# 附录1：pytorch的张量运算"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74910eb-f5cd-4ee7-ade1-038667405218",
   "metadata": {},
   "source": [
    "pytorch已成为当前最为流行的深度学习框架。它包括4个对深度学习十分重要的功能：张量运算（torch)、自动梯度（torch.autograd）、神经网络（torch.nn）和优化方法（torch.optim）。张量运算是我们深度学习编程的基础。本附录结合实际的例子来介绍有关pytorch张量运算的常用内容。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b232280c-8c20-4d27-82b0-778b94857566",
   "metadata": {},
   "outputs": [],
   "source": [
    "#使用pytorch库的张量运算需要首先在程序中引入该库\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de13262-ad70-4549-ae5b-c17d68101266",
   "metadata": {},
   "source": [
    "调用torch库中张量函数的方法有两种：  \n",
    "1. 作为torch库的函数调用，\n",
    "2. 作为张量类的函数调用。\n",
    "通常情况下这两种方法是等价。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b190551e-1610-4f8e-9fac-c8ad88197867",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.]])\n",
      "tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,3)\n",
    "y = torch.full((2,3), 2)\n",
    "#方法1：作为torch库的函数调用\n",
    "w = torch.add(x, y)\n",
    "#方法2：作为张量x类的类函数调用\n",
    "z = x.add(y)\n",
    "print(w)\n",
    "print(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b05972ad-c7c0-419b-aa7f-475036318146",
   "metadata": {},
   "source": [
    "作为类函数有两种版本：  \n",
    "1. 结尾带下划线\n",
    "2. 结尾不带下划线\n",
    "不带下划线的版本将运算结果作为返回值，不修改对象本身。带下划线的版本直接修改对象本身。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fe515c83-ab59-4c73-a92d-27c4eccb2064",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x=tensor([[1., 1., 1.],\n",
      "        [1., 1., 1.]])\n",
      "x=tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.]])\n",
      "z=tensor([[3., 3., 3.],\n",
      "        [3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "x = torch.ones(2,3)\n",
    "y = torch.full((2,3), 2)\n",
    "z = x.add(y)\n",
    "print(f\"x={x}\")\n",
    "z = x.add_(y)\n",
    "print(f\"x={x}\")\n",
    "print(f\"z={z}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22555538-696b-4ea6-b292-8589ae94a5f5",
   "metadata": {},
   "source": [
    "| 分类  | 函数 |  功能|\n",
    "| ---| --- | --- |\n",
    "|初始化张量 | torch.tensor | 出列表初始化 |\n",
    "|     |torch.empty| 产生一个元素初值不确定的张量 |\n",
    "|     |torch.zeros| 产生一个元素初值为0的张量 |\n",
    "|     |torch.ones | 产生一个元素初值为1的张量 |\n",
    "|     |torch.rand | 产生一个元素初值在（0, 1）之间均匀分布的张量|\n",
    "|     |torch.randn | 产生一个元素初值按照均值为0，方差为1的标准正态分布的张量|\n",
    "|     |torch.ones_like|产生一个形状和输入矩阵相同但元素初值为1的张量|\n",
    "|     |torch.randn_like|产生一个形状和输入矩阵相同但元素初值按照均值为0，方差为1的标准正态分布的张量|\n",
    "|     |torch.linespace|产生在指定区间（含端点）内等距取值的一维向量|\n",
    "|改变张量形状|torch.unsqueeze|在指定位置增加1个大小为1的维度|\n",
    "|          |torch.squeeze|在指定为减少1个维度，只能减少大小为1的维度|\n",
    "|          |torch.transpose|交换张量的两个维度|\n",
    "|          |torch.view|将张量改为指定形状，不改变内存中数据的位置|\n",
    "|          |torch.reshape|将张量改为指定形状，必要时改变内存中数据的位置|\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bbd39b6-6670-4020-980a-1c0cf77a62c7",
   "metadata": {},
   "source": [
    "## 1. 初始化张量  \n",
    "### 1.1 将python list转化为张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a99d547d-3d26-4264-8973-d9cf7e83a12d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 4])\n",
      "torch.float64\n",
      "tensor([[1., 2., 3., 4.],\n",
      "        [5., 6., 7., 8.],\n",
      "        [2., 4., 6., 8.],\n",
      "        [1., 3., 5., 7.]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor([[1,2,3,4],[5,6,7,8],[2,4,6,8],[1,3,5,7]], dtype=torch.float64)\n",
    "print(x.shape)\n",
    "print(x.dtype)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "152d93e0-de79-4244-8ce2-27de8a915a6a",
   "metadata": {},
   "source": [
    "### 1.2 利用函数生成张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e6b3acb2-52cb-470e-8600-ada1fdba9b46",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]],\n",
       "\n",
       "        [[0., 0.],\n",
       "         [0., 0.],\n",
       "         [0., 0.]]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.empty(2, 3, 2) #用empty函数生成未初始化的张量，张量中的元素取值是不确定的。\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "529863cd-41f8-4a51-a9d8-4a8d9b2d8500",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.zeros(4,2) #创建各元素都是0的张量\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9eb77e77-6be5-48da-a0da-9bdd81d09f98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 1, 1],\n",
       "        [1, 1, 1, 1],\n",
       "        [1, 1, 1, 1]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = torch.ones(3, 4, dtype=torch.int64) #创建各元素都是1的张量\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9da1c7c5-21d4-4af3-97dd-333d303336d6",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-1.2329,  0.0665],\n",
       "         [ 0.1421,  0.0609],\n",
       "         [-0.5853, -0.4087],\n",
       "         [ 1.2857,  1.4072]],\n",
       "\n",
       "        [[-1.5115, -0.5577],\n",
       "         [-1.6384, -0.1772],\n",
       "         [-1.0019,  2.2391],\n",
       "         [ 1.9776,  0.1737]]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = torch.randn(2,4,2) #创建元素取值符合均值为0，方差为1的标准高斯分布的张量\n",
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "211660aa-558b-4c9d-8e07-2b427e938414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7385, 0.3031, 0.6792],\n",
       "        [0.1450, 0.8152, 0.2549]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = torch.rand(2,3) #创建元素值在（0,1）之间且满足均匀分布的张量\n",
    "d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2672c9d4-0742-408e-a5e8-b6e9b5a46119",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1., 1., 1.],\n",
       "        [1., 1., 1.]])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e = torch.ones_like(d) #创建一个形状和d相同，元素值全为1的张量\n",
    "e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "df6ff041-3ef4-432d-a9ec-3b6401dd5fc6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.4571, -1.1267, -0.3898],\n",
       "        [-0.0902, -0.8235,  0.8499]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = torch.randn_like(d) #创建一个形状和d相同,元素值符合均值为0，方差为1的标准高斯分布\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "95db50a9-5035-4825-84ca-1510fabe36af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.0000,  3.2500,  5.5000,  7.7500, 10.0000])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.linspace(1, 10, 5) #生成1维张量，元素值从左到右依次从1到10的区间里等距离采样\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7d9be37d-6fc0-47bd-8824-85240175e2b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[3.1400, 3.1400, 3.1400],\n",
      "        [3.1400, 3.1400, 3.1400]])\n",
      "tensor([[3.1400, 3.1400, 3.1400],\n",
      "        [3.1400, 3.1400, 3.1400]])\n"
     ]
    }
   ],
   "source": [
    "#将张量初始化成任意值的方法\n",
    "h = torch.full((2, 3), 3.14) \n",
    "print(h)\n",
    "i = torch.empty((2, 3)).fill_(3.14)\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5771cb4a-cb87-4bb7-bb71-fcc1e5ee834a",
   "metadata": {},
   "source": [
    "## 2. 访问张量  \n",
    "通过索引访问张量中的元素或张量的一部分。torch支持十分灵活的索引表示。从下面的例子来感受吧。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "74c6cc8f-eed2-4e13-8544-d1850170672f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(0,10)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2c7b924e-7020-44c2-a9d3-5fe461d28178",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7)\n",
      "7\n"
     ]
    }
   ],
   "source": [
    "x = a[7]\n",
    "print(x)\n",
    "#当张量只包含一个元素时，用item函数返回这个值。注意比较两个输出在类型上的区别。\n",
    "print(x.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "412e4d71-6865-4ad5-96a2-8583cc918fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([4, 5, 6, 7])\n"
     ]
    }
   ],
   "source": [
    "b = a[4:8]\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "11a02413-7795-4f89-b77a-fb9a3434c5d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([7, 8])\n"
     ]
    }
   ],
   "source": [
    "c = a[7:-1]\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3619917a-0a61-43d0-829f-e6374a7c72ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6, 7, 8, 9])\n"
     ]
    }
   ],
   "source": [
    "d = a[6:]\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d4410f65-0fd0-4c46-8db3-6229433584e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1, 2, 3, 4, 5, 6, 7, 8])\n"
     ]
    }
   ],
   "source": [
    "e = a[:-1]\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "79bb4dc5-0d0f-440b-a044-98cde67723c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 10])\n"
     ]
    }
   ],
   "source": [
    "f = torch.randn(9,8,10)\n",
    "g = f[:,2,:]\n",
    "print(g.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3324c4f1-ceaf-45c4-ade4-73bda137634a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([9, 2, 9])\n"
     ]
    }
   ],
   "source": [
    "h = f[:,3:5,:-1]\n",
    "print(h.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99f453b5-a940-4bae-9896-1eda6b033b51",
   "metadata": {},
   "source": [
    "## 3. 改变张量的形状 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f6f7324-999d-4d40-8c25-4b27c3ebe093",
   "metadata": {},
   "source": [
    "### 3.1 增加张量的维度（torch.unsqueeze）\n",
    "在张量的指定位置增加一个大小为1的维度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "70bd674d-f7d4-403f-9b3e-4f1068770d82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 1])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4246],\n",
       "         [-0.9256],\n",
       "         [-1.3181]],\n",
       "\n",
       "        [[ 0.6276],\n",
       "         [ 0.6220],\n",
       "         [-0.0096]]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(2,3)\n",
    "b = a.unsqueeze(-1) #在张量a的最后增加一个维度，大小为1\n",
    "print(b.size())\n",
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "5a3a639a-7522-458f-a09a-5e7011e644c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.4246, -0.9256, -1.3181]],\n",
       "\n",
       "        [[ 0.6276,  0.6220, -0.0096]]])"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "c = a.unsqueeze(1) #在张量a的第1个维度之后增加一个维度，大小为1\n",
    "print(c.size())\n",
    "c"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b02dcef9-70d3-4b6d-bc3f-d0d9f05f9ca2",
   "metadata": {},
   "source": [
    "### 3.2 减少张量的维度（torch.squeeze)\n",
    "在张量的指定位置减少一个维度，要求被减少的维度大小为1。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "edde3b65-dc1c-4eb5-9137-0001c2764c26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2862,  0.8116,  0.8155],\n",
       "        [ 0.1872,  0.0728, -0.6693]])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d = b.squeeze(-1) #压缩张量b的最后1维。被压缩的维度对应的大小只能是1，否则不压缩\n",
    "print(d.size())\n",
    "d"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951c81ae-ccf1-4fb7-9dcd-879daa059fce",
   "metadata": {},
   "source": [
    "### 3.3 交换张量的维度（torch.tranpose）  \n",
    "交换张量的维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ca3c65c1-fdcd-42b3-993b-c6cd214edc18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 6, 9, 1, 3, 7])\n"
     ]
    }
   ],
   "source": [
    "p = torch.rand(5,6,3,1,9,7)\n",
    "p = p.transpose(2,4)\n",
    "print(p.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "d00e6b52-5abf-4cca-9c7c-6a5f23c81bfa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.2862,  0.8116],\n",
       "        [ 0.8155,  0.1872],\n",
       "        [ 0.0728, -0.6693]])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = e.view(3,2) #e的形状变为3,2\n",
    "f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "5825be5c-55ac-4d2d-ad2d-cfb856b9ff6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ 2.0603,  0.5122],\n",
       "         [-0.6771, -0.7321],\n",
       "         [ 0.1978,  0.5159]],\n",
       "\n",
       "        [[-1.7624,  1.2907],\n",
       "         [-1.9971, -0.2712],\n",
       "         [-0.7361, -0.3411]]])"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g = torch.randn(2,3,2)\n",
    "g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "152e370f-83c4-4f5e-9fb0-fd221661297c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 2.0603,  0.5122, -0.6771, -0.7321,  0.1978,  0.5159, -1.7624,  1.2907,\n",
       "        -1.9971, -0.2712, -0.7361, -0.3411])"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h = g.view(-1) #转变为1维张量\n",
    "h"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8091cdb1-15ab-4a54-a899-3e616afdb3ce",
   "metadata": {},
   "source": [
    "### 3.4 任意改变张量的形状（torch.view和torch.reshape）\n",
    "view和reshape都可以灵活的改变张量的形状，只要改变后的各维度大小相乘和改变前各维度大小相乘相等即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c7a3baaf-26c2-4c2a-9cb4-6458725b684b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 81, 2, 4])\n"
     ]
    }
   ],
   "source": [
    "p = torch.rand(5,162,8)\n",
    "q = p.view((10, 81, 2, 4))\n",
    "print(q.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "04e3c8ee-d15f-41e3-8c99-22a4edd13e1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([90, 9, 4, 2])\n"
     ]
    }
   ],
   "source": [
    "r = p.reshape((90, 9, 4, 2))\n",
    "print(r.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ad6e08-2580-4fcb-a9e3-c437f9d61e58",
   "metadata": {},
   "source": [
    "view和reshape改变张量形状时，并非真正改变了数据在内存中的位置，而是改变张量的索引。这样做减少了数据在内存的复制，因此比较高效，需要张量在变形之前索引是连续的。如果索引不是连续的，则只能使用reshape函数来改变张量的形状。关于这一点的具体解释可参见下面第8小节"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a18de21-cf53-4959-a196-2344fddefd66",
   "metadata": {},
   "source": [
    "## 4. 张量的合并和拆分"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9b4275-d53f-4174-bf0c-d2c0c3904a50",
   "metadata": {},
   "source": [
    "### 4.1 张量的合并（torch.cat和torch.stack）  \n",
    "torch.cat，多个张量沿着某个维度合并。要求合并的张量除了这个维度之外，其他维度的大小必须一样。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "243ce915-db26-4d31-8d92-93e38ff7c35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 7, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3,5,6)\n",
    "b = torch.randn(2,4,5,6)\n",
    "#使用cat函数，沿指定维度合并张量。把需要合并在张量放在一个tuple里。\n",
    "c = torch.cat((a, b), dim=1)\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae96d26-f331-43a2-9939-61478d848a12",
   "metadata": {},
   "source": [
    "torch.stack，沿着一个新的维度将多个张量合并。要求这些张量的形状必须完全一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "9825dbdd-b2c0-4652-8b11-d78ca0a8109c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 4, 5, 6])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(2,3,5,6)\n",
    "b = torch.randn(2,3,5,6)\n",
    "c = torch.randn(2,3,5,6)\n",
    "d = torch.randn(2,3,5,6)\n",
    "#将a, b, c, d 4个张量合并成新张量的第3个维度\n",
    "e = torch.stack((a, b, c, d), dim=2)\n",
    "print(e.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e39ecf-a523-45b6-ab21-4ee39312d113",
   "metadata": {},
   "source": [
    "### 4.2 torch.split  \n",
    "将张量沿某个维度，拆分成多个张量。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "22938edd-4a5e-403c-8797-837f121b36d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 10, 7])\n",
      "torch.Size([5, 50, 7])\n",
      "torch.Size([5, 40, 7])\n",
      "torch.Size([5, 62, 7])\n"
     ]
    }
   ],
   "source": [
    "p =torch.randn(5,162,7)\n",
    "a, b, c, d = torch.split(p, [10, 50, 40, 62], dim=1)\n",
    "print(a.size())\n",
    "print(b.size())\n",
    "print(c.size())\n",
    "print(d.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43615665-39ff-4969-8884-667726915a89",
   "metadata": {},
   "source": [
    "### 4.3 torch.chunk  \n",
    "将张量沿某个指定的维度切成若干块"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5999ec7c-0408-41fc-97b5-25ef515cfa4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 41, 7])\n",
      "torch.Size([5, 41, 7])\n",
      "torch.Size([5, 41, 7])\n",
      "torch.Size([5, 39, 7])\n"
     ]
    }
   ],
   "source": [
    "p =torch.randn(5,162,7)\n",
    "a, b, c, d = torch.chunk(p, 4, dim=1)\n",
    "print(a.size())\n",
    "print(b.size())\n",
    "print(c.size())\n",
    "print(d.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cf14c9d-63af-417d-8b04-d441f4483d29",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## 5. 广播机制\n",
    "广播机制用于解决形状不匹配的张量间进行运算的问题。概括的的说，广播机制会按照规则，逐一比较两个向量的各个维度，并通过复制的方式将较小的维度扩大到和较大维度相等，从而最终使两者形状一致。需要注意的是并非任意两个张量都可以通过广播变成形状一致。可以通过广播机制变为形状一致的张量需满足以下条件：  \n",
    "1. 两个张量都不能是0维的。\n",
    "2. 从右向左逐个维度比较两个张量。两个张量各对应维度需要满足以下条件之一  \n",
    "    . 相等  \n",
    "    . 其中一个张量的大小为1，可通过复制变为一致   \n",
    "    . 其中一个张量不存在这个维度，可通过插入维度在复制变为一致"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d8b9a4d8-7a1d-42fb-9bad-3149ccfcb125",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 5, 4, 3])\n"
     ]
    }
   ],
   "source": [
    "# a和b可通过广播变为形状一致：\n",
    "# 从右向左依次\n",
    "# 第1个维度，b维度大小是1\n",
    "# 第2个维度，a和b的维度大小相同\n",
    "# 第3个维度，a的维度大小是1\n",
    "# 第4个维度：b该维度不存在\n",
    "a = torch.randn(2, 1, 4, 3)\n",
    "b = torch.randn(   5, 4, 1)\n",
    "c = a + b\n",
    "print(c.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2477898-9d54-443c-b7d1-50c0d3876e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x和y不可通过广播变为形状一致\n",
    "# 因为从右边数第三个维度，两个张量该维度的大小不同且没有一个张量该维度的大小为1，无法通过复制的方式让它们的维度一致。\n",
    "x = torch.empty(2, 2, 4, 3)\n",
    "y = torch.empty(   5, 4, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "74356aae-2d8c-4ccf-92ae-34d8906d6d0f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 5])\n",
      "tensor([[[3.1000, 3.1000, 3.1000, 3.1000, 3.1000],\n",
      "         [3.1000, 3.1000, 3.1000, 3.1000, 3.1000]],\n",
      "\n",
      "        [[3.1000, 3.1000, 3.1000, 3.1000, 3.1000],\n",
      "         [3.1000, 3.1000, 3.1000, 3.1000, 3.1000]],\n",
      "\n",
      "        [[3.1000, 3.1000, 3.1000, 3.1000, 3.1000],\n",
      "         [3.1000, 3.1000, 3.1000, 3.1000, 3.1000]]])\n"
     ]
    }
   ],
   "source": [
    "# 利用广播机制完成张量加法\n",
    "a = torch.ones(3, 2, 1)\n",
    "b = torch.empty(5).fill_(2.1)\n",
    "c = a + b\n",
    "print(c.size())\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f853c382-5db1-4fb0-b553-3ede4ace72c7",
   "metadata": {},
   "source": [
    "## 6. 张量的乘法"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d537f66f-1364-4baf-b820-18ac5afed537",
   "metadata": {},
   "source": [
    "### 6.1 逐元素相乘（*）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "85e9ba6d-6640-4797-ae3d-f442fc477f84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6., 6., 6.],\n",
      "        [6., 6., 6.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.empty((2,3)).fill_(2.0)\n",
    "b = torch.empty((1,3)).fill_(3.0)\n",
    "#不要求输入两个张量形状一致，但两个张量需要能够通过广播机制变为形状一致。\n",
    "c = a * b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf69853-9069-40de-bd1b-addd70039110",
   "metadata": {},
   "source": [
    "### 6.2 向量点积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "218ab523-2490-4866-8e26-2d076c1d20a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(30)\n"
     ]
    }
   ],
   "source": [
    "a = torch.full((5,), 2)\n",
    "b = torch.full((5,), 3)\n",
    "#输入必须都是1维向量，且维度大小一致。\n",
    "c = torch.dot(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb796db-d216-477b-ac5b-787fa338c257",
   "metadata": {},
   "source": [
    "### 6.3 向量外积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "ce8a7b85-ec8f-46fd-9521-223c2450d12b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6, 6, 6],\n",
      "        [6, 6, 6],\n",
      "        [6, 6, 6]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.full((3,), 2)\n",
    "b = torch.full((3,), 3)\n",
    "#输入必须都是1维向量，且维度大小一致。\n",
    "c = torch.outer(a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "936817eb-862b-4fca-b9f8-180afcc863d1",
   "metadata": {},
   "source": [
    "### 6.4 矩阵乘法（@）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a3bf097f-4e32-4e72-ad5c-2e0b19e8ce91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18., 18.],\n",
      "        [18., 18.]])\n"
     ]
    }
   ],
   "source": [
    "#两个2维矩阵相乘，维度必须匹配，即前一个矩阵的列数必须和后一个矩阵的行数相等\n",
    "a = torch.empty((2,3)).fill_(2.0)\n",
    "b = torch.empty((2,3)).fill_(3.0)\n",
    "c = a @ b.transpose(0,1)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "cc01421a-1842-4466-ac43-38ea76b1f258",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([6., 6.])\n"
     ]
    }
   ],
   "source": [
    "#向量和矩阵相乘，相当于行向量和矩阵相乘。\n",
    "a = torch.tensor([1.0, 2.0, 3.0])\n",
    "b = torch.ones(3,2)\n",
    "c = a @ b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "675e4658-5a01-47f4-ad3f-523c097e7a33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5., 5., 5.])\n"
     ]
    }
   ],
   "source": [
    "#矩阵和向量相乘，相当于矩阵和列向量相乘\n",
    "a = torch.ones(3,2)\n",
    "b = torch.tensor([1.0,4.0])\n",
    "c = a @ b\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a2399e-4c90-44d3-b82e-9e75a53b9f81",
   "metadata": {},
   "source": [
    "### 6.5 批量矩阵乘法（@）\n",
    "当相乘的张量维度大于2时，执行批量矩阵乘法。对于形状为(..., m, n)的张量A和形状为(..., n, p)的张量B，批量矩阵乘法的结果是一个形状为(..., m, p)的张量。每个张量的\"...\"部分代表任意数量的批量维度。当进行批量乘法时，pytorch会利用广播机制自动让各向量批量维度一致。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "d4ccc329-108f-4109-be6b-2866f339d9a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 4, 3])\n",
      "tensor([[[ 0.1684,  0.9406, -1.7558],\n",
      "         [-2.6504,  1.8649,  2.7411],\n",
      "         [ 5.1198, -4.5485, -3.3203],\n",
      "         [ 3.4810, -6.1789, -4.1025]],\n",
      "\n",
      "        [[ 2.4855, -6.3756, -2.7549],\n",
      "         [-5.1606,  3.1573,  1.1740],\n",
      "         [-0.2549,  3.1623,  3.0671],\n",
      "         [ 2.1162, -5.3233, -1.2320]],\n",
      "\n",
      "        [[-0.1310,  3.3769, -0.3214],\n",
      "         [-3.0650,  1.6578, -0.8541],\n",
      "         [-0.9523,  0.0756, -0.4481],\n",
      "         [-1.1875,  2.6943,  2.6880]]])\n"
     ]
    }
   ],
   "source": [
    "d = torch.randn(3,4,5)\n",
    "e = torch.randn(1,5,3)\n",
    "f = d @ e\n",
    "print(f.size())\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64127c9e-3495-4134-9e82-b94d7d57f01e",
   "metadata": {},
   "source": [
    "### 6.6 利用矩阵乘法实现批量向量内积和外积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c2087d81-a8dc-49cd-9430-6966dcce49a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      "tensor([[-2.8519,  1.6670],\n",
      "        [ 3.2973,  4.3137],\n",
      "        [-0.4093,  0.5068]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3,2,5)\n",
    "b = torch.randn(3,2,5)\n",
    "#对a和b的最后一维做批量内积\n",
    "a = a.unsqueeze(-2) #a形状为(3, 2, 1, 5) \n",
    "b = b.unsqueeze(-1) #b形状为(3, 2, 5, 1)\n",
    "c = a @ b #c形状为(3, 2, 1, 1)\n",
    "c.squeeze_(-1).squeeze_(-1)\n",
    "print(c.size())\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "5f96fc76-a449-4e72-84be-660e4dc26427",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2, 2, 2])\n",
      "tensor([[[[-0.5924, -1.8389],\n",
      "          [-0.3113, -0.9664]],\n",
      "\n",
      "         [[ 0.7701, -0.0239],\n",
      "          [-0.1870,  0.0058]]],\n",
      "\n",
      "\n",
      "        [[[ 0.4913, -0.2648],\n",
      "          [-1.2844,  0.6923]],\n",
      "\n",
      "         [[-0.1414,  0.0411],\n",
      "          [-1.3858,  0.4028]]],\n",
      "\n",
      "\n",
      "        [[[-0.9607,  1.4274],\n",
      "          [ 1.1836, -1.7586]],\n",
      "\n",
      "         [[ 0.0319, -0.0793],\n",
      "          [ 0.5575, -1.3866]]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.randn(3,2,2)\n",
    "b = torch.randn(3,2,2)\n",
    "#对a和b的最后一维做批量外积\n",
    "a = a.unsqueeze(-1) #a形状为(3, 2, 2, 1) \n",
    "b = b.unsqueeze(-2) #b形状为(3, 2, 1, 2)\n",
    "c = a * b #这里利用了广播机制，c的形状为(3, 2, 2, 2)\n",
    "print(c.size())\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d927fc73-42a1-47e9-9741-01a060f716f9",
   "metadata": {},
   "source": [
    "## *7. 爱因斯坦求和约定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e8d56b9-c60e-4e2e-84c8-edfa13d6431f",
   "metadata": {},
   "source": [
    "1. 张量扩维: 'i, j -> ij' 相当于 $a^Tb$，也就是向量a和向量b的元素两两相乘得到一个矩阵。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "32de5c07-9154-48eb-874c-a86d7e8e9825",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6, 6],\n",
      "        [6, 6],\n",
      "        [6, 6]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.full((3,),2)\n",
    "b = torch.full((2,),3)\n",
    "c = torch.einsum('i, j -> ij', a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6725d1fd-6ae3-430c-bd05-ed11422f01d6",
   "metadata": {},
   "source": [
    "2.张量缩维: 'i->'相当于对该维度求和$\\sum_{i}a_{i}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "a1b69df3-024d-4a39-9569-4acc6fa0f12e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(6)\n"
     ]
    }
   ],
   "source": [
    "d = torch.full((3,),2)\n",
    "e = torch.einsum('i->', d)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd3d598-e373-4355-bd4e-23246def3e50",
   "metadata": {},
   "source": [
    "3.交换张量任意两个维度，不改变张量本身。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "6fdb59cd-a8fd-481c-8bed-d1548b2349e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.8026,  1.5833,  0.8638],\n",
      "        [-0.7444,  0.3484,  0.2103]])\n",
      "tensor([[ 0.8026, -0.7444],\n",
      "        [ 1.5833,  0.3484],\n",
      "        [ 0.8638,  0.2103]])\n"
     ]
    }
   ],
   "source": [
    "f = torch.randn(2,3)\n",
    "g = torch.einsum('ij->ji', f)\n",
    "print(f)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c9dc2c4-b971-4396-bcea-0c39f6a3a013",
   "metadata": {},
   "source": [
    "矩阵乘法 'ij, jk -> ik' 相当于 'ij -> ji' 'ji, jk -> jik' 'jik->ik'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "777bf88e-ba9d-4f8b-9fe7-a5a8a0521199",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[18, 18],\n",
      "        [18, 18]])\n",
      "tensor([[18, 18],\n",
      "        [18, 18]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.full((2,3),2)\n",
    "b = torch.full((3,2),3)\n",
    "c = a @ b\n",
    "d = torch.einsum('ij, jk -> ik', a, b)\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d246991b-3816-4b8d-a8fa-e5b948dc4982",
   "metadata": {},
   "source": [
    "向量点积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "e6376b68-ddf3-4467-9262-cef5cc09353f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(18)\n",
      "tensor(18)\n"
     ]
    }
   ],
   "source": [
    "a = torch.full((3,),2)\n",
    "b = torch.full((3,),3)\n",
    "c = torch.dot(a,b)\n",
    "d = torch.einsum('i,i ->', a, b)\n",
    "print(c)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73c27e79-51b5-415e-b0c8-3e49b9c58a7c",
   "metadata": {},
   "source": [
    "向量外积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "8c74e7fc-ff5c-47be-9f4c-e6f7ebc1fffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6, 6, 6],\n",
      "        [6, 6, 6],\n",
      "        [6, 6, 6]])\n"
     ]
    }
   ],
   "source": [
    "d = torch.einsum('i,j -> ij', a, b)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7cb22d2a-639d-42f1-b271-d881d4cdbb7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 3, 4])\n",
      "tensor([[[[1.1033, 0.9944, 1.2414, 1.3750],\n",
      "          [0.9386, 0.8082, 0.9610, 1.0845],\n",
      "          [1.0565, 0.9615, 1.2828, 1.5182]],\n",
      "\n",
      "         [[0.8547, 1.0837, 1.1938, 1.1037],\n",
      "          [1.0152, 1.5503, 1.7114, 1.7363],\n",
      "          [0.5776, 1.0309, 0.8430, 0.7982]],\n",
      "\n",
      "         [[1.7249, 1.2541, 1.3379, 1.1049],\n",
      "          [2.3963, 1.7822, 1.7943, 1.4393],\n",
      "          [1.2522, 0.7162, 0.6649, 0.5888]]],\n",
      "\n",
      "\n",
      "        [[[0.5027, 0.4516, 0.5740, 0.2824],\n",
      "          [0.7352, 0.7604, 0.6649, 0.3975],\n",
      "          [1.1169, 1.0645, 1.3814, 0.6596]],\n",
      "\n",
      "         [[1.2100, 1.2466, 1.5792, 0.5028],\n",
      "          [0.9537, 0.9218, 1.3609, 0.5546],\n",
      "          [0.8692, 1.0916, 1.3718, 0.3503]],\n",
      "\n",
      "         [[1.7999, 1.6976, 1.2891, 2.0228],\n",
      "          [0.6799, 0.5010, 0.2203, 0.3707],\n",
      "          [0.7169, 1.0464, 0.4333, 1.2074]]]])\n"
     ]
    }
   ],
   "source": [
    "#两个张量最后一维做外积，再沿着l维求和。\n",
    "a = torch.rand(2, 3, 5, 3)\n",
    "b = torch.rand(2, 3, 5, 4)\n",
    "c = torch.einsum('bhlk, bhlv->bhkv', a, b)\n",
    "print(c.shape)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75d5b517-d2bd-4f6e-a6ab-078f09cc2dfe",
   "metadata": {},
   "source": [
    "下面这段代码通过基本的张量运算达到和上面爱因斯坦求和约定相同的效果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "2af8da59-6c4e-4d88-9459-df5acf36f00f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3, 5, 3, 1])\n",
      "torch.Size([2, 3, 5, 1, 4])\n",
      "torch.Size([2, 3, 3, 4])\n",
      "tensor([[[[1.1033, 0.9944, 1.2414, 1.3750],\n",
      "          [0.9386, 0.8082, 0.9610, 1.0845],\n",
      "          [1.0565, 0.9615, 1.2828, 1.5182]],\n",
      "\n",
      "         [[0.8547, 1.0837, 1.1938, 1.1037],\n",
      "          [1.0152, 1.5503, 1.7114, 1.7363],\n",
      "          [0.5776, 1.0309, 0.8430, 0.7982]],\n",
      "\n",
      "         [[1.7249, 1.2541, 1.3379, 1.1049],\n",
      "          [2.3963, 1.7822, 1.7943, 1.4393],\n",
      "          [1.2522, 0.7162, 0.6649, 0.5888]]],\n",
      "\n",
      "\n",
      "        [[[0.5027, 0.4516, 0.5740, 0.2824],\n",
      "          [0.7352, 0.7604, 0.6649, 0.3975],\n",
      "          [1.1169, 1.0645, 1.3814, 0.6596]],\n",
      "\n",
      "         [[1.2100, 1.2466, 1.5792, 0.5028],\n",
      "          [0.9537, 0.9218, 1.3609, 0.5546],\n",
      "          [0.8692, 1.0916, 1.3718, 0.3503]],\n",
      "\n",
      "         [[1.7999, 1.6976, 1.2891, 2.0228],\n",
      "          [0.6799, 0.5010, 0.2203, 0.3707],\n",
      "          [0.7169, 1.0464, 0.4333, 1.2074]]]])\n"
     ]
    }
   ],
   "source": [
    "a.unsqueeze_(-1)\n",
    "b.unsqueeze_(-2)\n",
    "print(a.shape)\n",
    "print(b.shape)\n",
    "d = (a * b).sum(-3)\n",
    "print(d.shape)\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "66dc3b54-b28d-4eca-8310-c738c25765a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[1.5000, 1.5000, 1.5000],\n",
      "         [1.5000, 1.5000, 1.5000],\n",
      "         [1.5000, 1.5000, 1.5000]],\n",
      "\n",
      "        [[1.5000, 1.5000, 1.5000],\n",
      "         [1.5000, 1.5000, 1.5000],\n",
      "         [1.5000, 1.5000, 1.5000]]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.ones(2,3,3)\n",
    "b = torch.empty(2,3,3).fill_(1.5)\n",
    "c = torch.einsum('ijk, ijk -> ijk', a, b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d49cdb-d9f3-4ac8-8b03-f0a05015a01f",
   "metadata": {},
   "source": [
    "用einsum函数重写Qwen3-next-80B源码文件module_qwen3_next.py中函数torch_recurrent_gated_delta_rule中的代码\n",
    "```python{inline-numbers}\n",
    "last_recurrent_state = last_recurrent_state + k_t.unsqueeze(-1) * delta.unsqueeze(-2)\n",
    "```\n",
    "重写后的形式为。如果熟悉爱因斯坦求和约定，这样改写就会比上面的写法简明很多。\n",
    "```python\n",
    "last_recurrent_state +=  torch.einsum('bhlk, bhlv -> bhkv', k_t, delta）\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6ba35b6-7d00-4bed-9006-0a9b6dc34cac",
   "metadata": {},
   "source": [
    "## 8. 张量在内存中的存储\n",
    "张量的数据在内存中是连续存储的。张量的形状只是不同索引方式而已。不同张量可以指向同一个存储区。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50d0dbc0-7325-451f-9efa-6265fc8d5a7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.4519, -0.9555,  0.9411],\n",
       "        [-0.7038, -0.3873, -0.5926]])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn((2,3))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b4205c5e-1cf8-43f2-89c6-15c4cbe95511",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4675281824\n",
      "4675281824\n"
     ]
    }
   ],
   "source": [
    "b = a\n",
    "print(id(a)) #id函数返回张量指向的地址空间\n",
    "print(id(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "ba57af38-801c-4c79-b804-52ed4be3bf63",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       " 0.4519212543964386\n",
       " -0.9555278420448303\n",
       " 0.9411177635192871\n",
       " -0.7038334608078003\n",
       " -0.38732263445854187\n",
       " -0.5925939679145813\n",
       "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "storage = a.storage()#storage函数返回张量存储区是实际内容\n",
    "storage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "81b3002a-4fc2-4d9f-9faf-db2a6a6dba51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 2])\n",
      " 0.4519212543964386\n",
      " -0.9555278420448303\n",
      " 0.9411177635192871\n",
      " -0.7038334608078003\n",
      " -0.38732263445854187\n",
      " -0.5925939679145813\n",
      "[torch.storage.TypedStorage(dtype=torch.float32, device=cpu) of size 6]\n"
     ]
    }
   ],
   "source": [
    "#view函数只是改变了张量的索引方式，并未改变张量在内存中存储的方式\n",
    "c = a.view(3, 2)\n",
    "print(c.size())\n",
    "print(c.storage())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45975ab-50fb-402f-b643-62587f7db7b6",
   "metadata": {},
   "source": [
    "如果我们把张量中所有元素的索引按照下面规则排成一行——按照维度从左向右，比较索引对应的下标值，标值小的排在前面。这时，如果该索引序列和每个索引对饮元素在内存中实际排列一致，我们就说索引是连续的。view函数和reshape函数虽然改变张量的形状，其实只是改变了索引结构，不会破坏索引的连续性，也不需要改变元素在内存中的位置。有些张量变换，比如交换张量维度，会破坏张索引的连续性。一旦索引的连续性被破坏就不能再使用view函数来改变张量的形状，必须使用reshape函数。这时候reshape函数会通过改变内存中元素的位置来回复索引的连续性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "5b2889ac-31ed-4fab-abf0-2f831e8d2d4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "torch.Size([3, 10, 4])\n",
      "True\n",
      "torch.Size([3, 4, 2, 5])\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "#函数is_contiguous用于判断当前张量的索引是否连续。\n",
    "a = torch.randn(3,5,2,4)\n",
    "print(a.is_contiguous())\n",
    "b = a.view(3,10,4)\n",
    "print(b.size())\n",
    "print(b.is_contiguous())\n",
    "c = a.transpose(1,3) #破坏了数据的连续性\n",
    "print(c.size())\n",
    "print(c.is_contiguous())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89d509f9-f4fe-404d-9a3a-e7228ac26f80",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LLM",
   "language": "python",
   "name": "llm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
